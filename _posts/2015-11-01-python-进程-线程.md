---
layout: post 
category: python
tags: [ 笔记 , python , 进程 线程 ]
---
{% include JB/setup %}

##多进程

###fork()
* 自动把`当前进程`（称为父进程）`复制`了一份（称为子进程），然后，分别在父进程和子进程内`返回`。`子进程`永远返回`0`，而`父进程`返回`子进程的ID`

###multiprocessing
* `multiprocessing`模块封装了`fork()`调用,multiprocessing需要`“模拟”`出`fork`的效果，父进程所有Python对象都必须通过`pickle`序列化再传到`子进程`去，所有，如果multiprocessing在Windows下调用失败了，要先考虑是不是pickle失败了。
* `multiprocessing`模块提供了一个`Process类`来代表一个`进程对象`
{% highlight python %}
from multiprocessing import Process
import os
# 子进程要执行的代码
def run_proc(name):
    print('Run child process %s (%s)...' % (name, os.getpid()))

if __name__=='__main__':
    print('Parent process %s.' % os.getpid())
    p = Process(target=run_proc, args=('test',))
    print('Child process will start.')
    p.start()
    p.join()
    print('Child process end.')
{% endhighlight %}
创建`子进程`时，只需要传入一个`执行函数`和函数的`参数`，创建一个`Process实例`，用`start()`方法启动,`join()`方法可以等待`子进程结束`后再继续往下运行，通常用于`进程间的同步`。

####Pool 进程池
{% highlight python %}
from multiprocessing import Pool
import os, time, random

def long_time_task(name):
    print('Run task %s (%s)...' % (name, os.getpid()))
    start = time.time()
    time.sleep(random.random() * 3)
    end = time.time()
    print('Task %s runs %0.2f seconds.' % (name, (end - start)))

if __name__=='__main__':
    print('Parent process %s.' % os.getpid())
    p = Pool(4)
    for i in range(5):
        p.apply_async(long_time_task, args=(i,))
    print('Waiting for all subprocesses done...')
    p.close()
    p.join()
    print('All subprocesses done.')
{% endhighlight %}
`Pool`对象调用`join()`方法会等待所有子进程执行完毕，调用`join()`之`前`必须先调用`close()`，调用close()之后就`不能`继续添加`新的Process`了。task 0，1，2，3是立刻执行的，而task 4要`等待`前面某个task完成后才执行，这是因为Pool的大小设置为4

###子进程 subprocess
* `subprocess` 启动一个子进程，然后控制其输入和输出。
{% highlight python %}
import subprocess

print('$ nslookup www.python.org')
r = subprocess.call(['nslookup', 'www.python.org'])
print('Exit code:', r)
{% endhighlight %}
* 如果子进程还需要`输入`,使用`communicate()`
{% highlight python %}
import subprocess

print('$ nslookup')
p = subprocess.Popen(['nslookup'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
output, err = p.communicate(b'set q=mx\npython.org\nexit\n')
print(output.decode('utf-8'))
print('Exit code:', p.returncode)
'''上面的代码相当于在命令行执行命令nslookup，然后手动输入
set q=mx
python.org
exit
'''
{% endhighlight %}

####进程间通信
* Python的`multiprocessing`模块包装了底层的机制，提供了`Queue`、`Pipes`等多种方式来交换数据。

{% highlight python %}
from multiprocessing import Process, Queue
import os, time, random

# 写数据进程执行的代码:
def write(q):
    print('Process to write: %s' % os.getpid())
    for value in ['A', 'B', 'C']:
        print('Put %s to queue...' % value)
        q.put(value)
        time.sleep(random.random())

# 读数据进程执行的代码:
def read(q):
    print('Process to read: %s' % os.getpid())
    while True:
        value = q.get(True)
        print('Get %s from queue.' % value)

if __name__=='__main__':
    # 父进程创建Queue，并传给各个子进程：
    q = Queue()
    pw = Process(target=write, args=(q,))
    pr = Process(target=read, args=(q,))
    # 启动子进程pw，写入:
    pw.start()
    # 启动子进程pr，读取:
    pr.start()
    # 等待pw结束:
    pw.join()
    # pr进程里是死循环，无法等待其结束，只能强行终止:
    pr.terminate()
{% endhighlight %}

##多线程
* `_thread`和`threading`，_thread是低级模块，threading是高级模块，对_thread进行了`封装`
{% highlight python %}
import time, threading

# 新线程执行的代码:
def loop():
    print('thread %s is running...' % threading.current_thread().name)
    n = 0
    while n < 5:
        n = n + 1
        print('thread %s >>> %s' % (threading.current_thread().name, n))
        time.sleep(1)
    print('thread %s ended.' % threading.current_thread().name)

print('thread %s is running...' % threading.current_thread().name)
t = threading.Thread(target=loop, name='LoopThread')
t.start()
t.join()
print('thread %s ended.' % threading.current_thread().name)
{% endhighlight %}
由于任何进程`默认`就会启动一个线程，我们把该线程称为`主线程`，主线程又可以启动新的线程，Python的threading模块有个`current_thread()`函数，它永远返回`当前线程`的实例。主线程实例的名字叫MainThread，子线程的名字在创建时指定，我们用LoopThread命名子线程

###LOCK
* 多线程和多进程最大的不同在于，`多进程中，同一个变量，各自有一份拷贝存在于每个进程中，互不影响`，而`多线程中，所有变量都由所有线程共享`，所以，任何一个变量都可以被任何一个线程修改，因此，线程之间共享数据最大的危险在于多个线程同时改一个变量，把内容给改乱了。
* 当`多个线程`同时执行`lock.acquire()`时，只有一个线程能成功地获取`锁`，然后`继续`执行代码，其他线程就继续`等待`直到获得锁为止。
{% highlight python %}
balance = 0
lock = threading.Lock()

def run_thread(n):
    for i in range(100000):
        # 先要获取锁:
        lock.acquire()
        try:
            # 放心地改吧:
            change_it(n)
        finally:
            # 改完了一定要释放锁:
            lock.release()
{% endhighlight %}

###多核CPU GIL锁
{% highlight python %}
import threading, multiprocessing

def loop():
    x = 0
    while True:
        x = x ^ 1

for i in range(multiprocessing.cpu_count()):
    t = threading.Thread(target=loop)
    t.start()
{% endhighlight %}
启动与CPU核心数量相同的N个线程，在4核CPU上可以监控到CPU占用率仅有102%，也就是仅使用了`一核`。但是用C、C++或Java来改写相同的死循环，直接可以把全部核心跑满，4核就跑到400%，8核就跑到800%

* `Global Interpreter Lock`,任何Python线程执行前，必须先获得`GIL锁`,然后，每执行100条字节码，`解释器`就自动释放`GIL锁`，让别的`线程`有机会执行。这个`GIL全局锁`实际上把`所有线程`的执行代码都给`上了锁`，所以，`多线程`在Python中只能`交替执行`，即使100个线程跑在100核CPU上，也只能用到`1个核`
* Python虽然`不`能利用`多线程`实现`多核任务`，但可以通过`多进程`实现多核任务。`多个Python进程`有各自独立的`GIL锁`，互不影响。

###线程局部变量 ThreadLocal实现
* 一个线程使用自己的局部变量比使用全局变量好，因为局部变量只有线程自己能看见，不会影响其他线程

{% highlight python %}
import threading

# 创建全局ThreadLocal对象:
local_school = threading.local()

def process_student():
    # 获取当前线程关联的student:
    std = local_school.student
    print('Hello, %s (in %s)' % (std, threading.current_thread().name))

def process_thread(name):
    # 绑定ThreadLocal的student:
    local_school.student = name
    process_student()

t1 = threading.Thread(target= process_thread, args=('Alice',), name='Thread-A')
t2 = threading.Thread(target= process_thread, args=('Bob',), name='Thread-B')
t1.start()
t2.start()
t1.join()
t2.join()
{% endhighlight %}
`全局变量`local_school就是一个ThreadLocal`对象`，每个Thread对它都可以`读写`student`属性`，但`互不影响`。你可以把local_school看成全局变量，但每个属性如local_school.student都是线程的局部变量，可以任意读写而互不干扰，也不用管理锁的问题，ThreadLocal内部会处理。可以理解为全局变量local_school是一个`dict`，不但可以用local_school.student，还可以绑定`其他变量`，如local_school.teacher等等。

* ThreadLocal最常用的地方就是为每个`线程`绑定一个数据库连接，HTTP请求，用户身份信息等，这样一个线程的所有调用到的处理函数都可以非常方便地访问这些资源。

###进程 vs. 线程
* 多进程模式最大的优点就是`稳定性高`，因为一个子进程崩溃了，不会影响主进程和其他子进程, 缺点是创建进程的`代价大`,同时运行的进程数也是有限的
* 多线程模式通常比多进程`快`一点，但是也快不到哪去，而且，多线程模式致命的`缺点`就是任何一个线程挂掉都可能`直接`造成`整个`进程`崩溃`，因为所有线程共享进程的内存。

###分布式进程
* Process可以分布到多台机器上，而Thread最多只能分布到同一台机器的多个CPU
* Python的`multiprocessing`模块不但支持多进程，其中`managers`子模块还支持把`多进程`分布到`多台机器`上。一个`服务进程`可以作为调度者，将任务分布到`其他多个进程`中
<br clear="both">
例子

* 服务进程负责启动Queue，把Queue注册到网络上，然后往Queue里面写入任务
{% highlight python %}
# task_master.py

import random, time, queue
from multiprocessing.managers import BaseManager

# 发送任务的队列:
task_queue = queue.Queue()
# 接收结果的队列:
result_queue = queue.Queue()

# 从BaseManager继承的QueueManager:
class QueueManager(BaseManager):
    pass

# 把两个Queue都注册到网络上, callable参数关联了Queue对象:
QueueManager.register('get_task_queue', callable=lambda: task_queue)
QueueManager.register('get_result_queue', callable=lambda: result_queue)
# 绑定端口5000, 设置验证码'abc':
manager = QueueManager(address=('', 5000), authkey=b'abc')
# 启动Queue:
manager.start()
# 获得通过网络访问的Queue对象:
task = manager.get_task_queue()
result = manager.get_result_queue()
# 放几个任务进去:
for i in range(10):
    n = random.randint(0, 10000)
    print('Put task %d...' % n)
    task.put(n)
# 从result队列读取结果:
print('Try get results...')
for i in range(10):
    r = result.get(timeout=10)
    print('Result: %s' % r)
# 关闭:
manager.shutdown()
print('master exit.')
{% endhighlight %}
* 然后，在另一台机器上启动任务进程（本机上启动也可以)
{% highlight python %}
# task_worker.py

import time, sys, queue
from multiprocessing.managers import BaseManager

# 创建类似的QueueManager:
class QueueManager(BaseManager):
    pass

# 由于这个QueueManager只从网络上获取Queue，所以注册时只提供名字:
QueueManager.register('get_task_queue')
QueueManager.register('get_result_queue')

# 连接到服务器，也就是运行task_master.py的机器:
server_addr = '127.0.0.1'
print('Connect to server %s...' % server_addr)
# 端口和验证码注意保持与task_master.py设置的完全一致:
m = QueueManager(address=(server_addr, 5000), authkey=b'abc')
# 从网络连接:
m.connect()
# 获取Queue的对象:
task = m.get_task_queue()
result = m.get_result_queue()
# 从task队列取任务,并把结果写入result队列:
for i in range(10):
    try:
        n = task.get(timeout=1)
        print('run task %d * %d...' % (n, n))
        r = '%d * %d = %d' % (n, n, n*n)
        time.sleep(1)
        result.put(r)
    except Queue.Empty:
        print('task queue is empty.')
# 处理结束:
print('worker exit.')
{% endhighlight %}
注意Queue的作用是用来`传递任务`和`接收结果`，每个任务的描述数据量要`尽量小`。比如发送一个处理日志文件的任务，就不要发送几百兆的日志文件本身，而是发送日志文件`存放的完整路径`，由Worker进程再去共享的磁盘上`读取文件`。





